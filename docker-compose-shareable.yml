version: '3.8'

services:
  # Ollama LLM Service - Core AI model provider
  ollama:
    image: ollama/ollama:latest
    container_name: memory_ollama_shared
    ports:
      - '11434:11434'
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    networks:
      - memory_network
    healthcheck:
      test: ['CMD', 'ollama', 'list']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    # Resource limits for better compatibility
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

  # Memory MCP Server - Your brain-enhanced AI system
  memory_mcp_server:
    build:
      context: .
      dockerfile: Dockerfile.shareable
    container_name: memory_mcp_server_shared
    # Use stdio for MCP communication (no ports needed)
    stdin_open: true
    tty: true
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=phi3:mini
      - MEMORY_STORAGE_DIR=/app/brain_memory_store
      - DEBUG_MODE=false
      - MCP_LOG_LEVEL=INFO
      - PYTHONUNBUFFERED=1
    volumes:
      - ./brain_memory_store:/app/brain_memory_store
      - ./logs:/app/logs
      - brain_database:/app/database
      - ./plugins:/app/plugins
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - memory_network
    # Health check for the MCP server
    healthcheck:
      test: ['CMD', 'python', '-c', 'import sys; sys.exit(0)']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Model Setup Service - Downloads required AI models
  model_setup:
    image: ollama/ollama:latest
    container_name: memory_model_setup_shared
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - memory_network
    environment:
      - OLLAMA_HOST=ollama:11434
    command: ['pull', 'phi3:mini']
    restart: 'no'

  # Optional: Web UI for direct LLM interaction and testing
  ollama_webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: memory_ollama_webui_shared
    ports:
      - '3000:8080'
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_SECRET_KEY=memory-ai-2024
      - DEFAULT_MODELS=phi3:mini
    volumes:
      - ollama_webui_data:/app/backend/data
    depends_on:
      - ollama
    restart: unless-stopped
    networks:
      - memory_network
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8080/api/v1/chat/models']
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

volumes:
  ollama_data:
    name: memory_ollama_data_shared
  ollama_webui_data:
    name: memory_ollama_webui_data_shared
  brain_database:
    name: memory_brain_database_shared

networks:
  memory_network:
    name: memory_network_shared
    driver: bridge
